{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim, logging\n",
    "from gensim import corpora, models, similarities\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "files1 = []\n",
    "for i, file in enumerate(os.listdir('C://Users//asus//Desktop//Cybercorpora//PoliticalScienceandIR')):\n",
    "    jj = os.path.abspath('Desktop\\\\Cybercorpora\\\\PoliticalScienceandIR\\\\' + file)\n",
    "    dd = re.sub(r'\\\\', '//', jj)\n",
    "    xx = open(jj, 'r', encoding = 'utf-8')\n",
    "    ii = xx.read()\n",
    "    files1.append(ii)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('russian')\n",
    "stopwords += ['а', 'б', 'в', 'г', 'д', 'е', 'ё', 'ж', 'з', 'и', 'й', 'к', 'л', 'м', 'н', 'о', 'п', 'р', 'c', 'т', 'у', 'ф', 'х', 'ц', 'ч', 'ш', 'щ', 'ъ', 'ы', 'ь', 'э', 'ю', 'я', 'гг']\n",
    "files = []\n",
    "for tryingtoclean in files1:\n",
    "    tryingtoclean = str(tryingtoclean)\n",
    "    tryingtoclean = tryingtoclean.lower()\n",
    "    tryingtoclean = re.sub('^\\w\\s', '', tryingtoclean)\n",
    "    tryingtoclean = re.sub(r'^[а-яА-Я]', '', tryingtoclean)\n",
    "    tryingtoclean = re.sub(r'[a-zA-z]', '', tryingtoclean)\n",
    "    tryingtoclean = re.sub(r'_', '', tryingtoclean)\n",
    "    tryingtoclean = re.sub(r'\\d', '', tryingtoclean)\n",
    "    tryingtoclean = re.sub(r'[ñâÿçàíñîáîðûɫɥɨêɜɨïïëèåòè]', '', tryingtoclean)\n",
    "    words = re.findall(r'\\w+', tryingtoclean)\n",
    "    clean = filter(lambda a: a not in stopwords, words)\n",
    "    x = ' '.join(clean).rstrip()\n",
    "    files.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [[word for word in file.lower().split()] for file in files]\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "texts = [[token for token in text if frequency[token] > 1] for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-20 23:26:30,534 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-03-20 23:26:41,938 : INFO : built Dictionary(235237 unique tokens: ['направленности', 'світовій', 'обслуживающим', 'краудфандингом', 'продуцированию']...) from 4751 documents (total 9254645 corpus positions)\n",
      "2018-03-20 23:26:42,030 : INFO : saving Dictionary object under C:/Users/asus/Desktop/Cybercorpora/Vectors/PSIRcorpus.dict, separately None\n",
      "2018-03-20 23:26:42,247 : INFO : saved C:/Users/asus/Desktop/Cybercorpora/Vectors/PSIRcorpus.dict\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(235237 unique tokens: ['направленности', 'світовій', 'обслуживающим', 'краудфандингом', 'продуцированию']...)\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.save('C:/Users/asus/Desktop/Cybercorpora/Vectors/PSIRcorpus.dict')\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-20 23:27:25,141 : INFO : storing corpus in Matrix Market format to C:/Users/asus/Desktop/Cybercorpora/Vectors/PSIRcorpus.mm\n",
      "2018-03-20 23:27:25,143 : INFO : saving sparse matrix to C:/Users/asus/Desktop/Cybercorpora/Vectors/PSIRcorpus.mm\n",
      "2018-03-20 23:27:25,144 : INFO : PROGRESS: saving document #0\n",
      "2018-03-20 23:27:26,904 : INFO : PROGRESS: saving document #1000\n",
      "2018-03-20 23:27:28,649 : INFO : PROGRESS: saving document #2000\n",
      "2018-03-20 23:27:30,262 : INFO : PROGRESS: saving document #3000\n",
      "2018-03-20 23:27:31,838 : INFO : PROGRESS: saving document #4000\n",
      "2018-03-20 23:27:33,050 : INFO : saved 4751x235237 matrix, density=0.482% (5388678/1117610987)\n",
      "2018-03-20 23:27:33,070 : INFO : saving MmCorpus index to C:/Users/asus/Desktop/Cybercorpora/Vectors/PSIRcorpus.mm.index\n"
     ]
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpora.MmCorpus.serialize('C:/Users/asus/Desktop/Cybercorpora/Vectors/PSIRcorpus.mm', corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-03-20 23:35:56,755 : INFO : using serial LSI version on this node\n",
      "2018-03-20 23:35:56,756 : INFO : updating model with new documents\n",
      "2018-03-20 23:35:56,758 : INFO : preparing a new chunk of documents\n",
      "2018-03-20 23:35:58,270 : INFO : using 100 extra samples and 2 power iterations\n",
      "2018-03-20 23:35:58,272 : INFO : 1st phase: constructing (235237, 101) action matrix\n",
      "2018-03-20 23:35:59,261 : INFO : orthonormalizing (235237, 101) action matrix\n",
      "2018-03-20 23:36:08,564 : INFO : 2nd phase: running dense svd on (101, 4751) matrix\n",
      "2018-03-20 23:36:08,744 : INFO : computing the final decomposition\n",
      "2018-03-20 23:36:08,745 : INFO : keeping 1 factors (discarding 68.333% of energy spectrum)\n",
      "2018-03-20 23:36:08,806 : INFO : processed documents up to #4751\n",
      "2018-03-20 23:36:08,813 : INFO : topic #0(3645.987): 0.702*\"è\" + 0.353*\"ò\" + 0.311*\"å\" + 0.188*\"ï\" + 0.166*\"ê\" + 0.162*\"ä\" + 0.143*\"ì\" + 0.107*\"ã\" + 0.103*\"ïëèòè\" + 0.103*\"ë\"\n"
     ]
    }
   ],
   "source": [
    "lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.548591935506581)]\n"
     ]
    }
   ],
   "source": [
    "doc = open('C:/Users/asus/Desktop/Cybercorpora/PoliticalScienceandIR/politicheskaya-propaganda-radikalnyh-islamistskih-organizatsiy-v-ssha.txt', 'r', encoding = 'utf-8').read()\n",
    "tryingtoclean = str(tryingtoclean)\n",
    "tryingtoclean = tryingtoclean.lower()\n",
    "tryingtoclean = re.sub('^\\w\\s', '', tryingtoclean)\n",
    "tryingtoclean = re.sub(r'^[а-яА-Я]', '', tryingtoclean)\n",
    "tryingtoclean = re.sub(r'[a-zA-z]', '', tryingtoclean)\n",
    "tryingtoclean = re.sub(r'_', '', tryingtoclean)\n",
    "tryingtoclean = re.sub(r'\\d', '', tryingtoclean)\n",
    "tryingtoclean = re.sub(r'[ñâÿçàíñîáîðûɫɥɨêɜɨïïëèåòè]', '', tryingtoclean)\n",
    "words = re.findall(r'\\w+', tryingtoclean)\n",
    "clean = filter(lambda a: a not in stopwords, words)\n",
    "x = ' '.join(clean).rstrip()\n",
    "vec_bow = dictionary.doc2bow(x.split())\n",
    "vec_lsi = lsi[vec_bow]\n",
    "print(vec_lsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.548591935506581)]\n"
     ]
    }
   ],
   "source": [
    "doc = open('C:/Users/asus/Desktop/Cybercorpora/PoliticalScienceandIR/politicheskaya-propaganda-radikalnyh-islamistskih-organizatsiy-v-ssha.txt', 'r', encoding = 'utf-8').read()\n",
    "vec_bow = dictionary.doc2bow(x.split())\n",
    "vec_lsi = lsi[vec_bow]\n",
    "print(vec_lsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
